{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Spell checker from Peter Norvig. Not perfect, but works.\n",
    "import re, collections\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = train(words(open('C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/big.txt').read()))\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import all modules needed\n",
    "## lem is lemmatizing the words\n",
    "from nltk.corpus import wordnet\n",
    "def lem(x):\n",
    "    sentence = ''\n",
    "    for x in re.sub('[^a-z0-9 ]','',x).split():   \n",
    "        sentence = sentence + ' ' + lemmatizer.lemmatize(correct(x))\n",
    "    count = 0\n",
    "    return sentence\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline\n",
    "from nltk import word_tokenize\n",
    "class LemmaTokenizer(object):\n",
    "     def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=10,stop_words=stops,max_features=1000,ngram_range=(1,3),tokenizer=LemmaTokenizer())\n",
    "tfvectorizer = TfidfVectorizer(stop_words=stops,min_df=10,max_features=1000,ngram_range=(1,3),tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take note of the assumptions made in the vectorizer specifications. There are two kinds of vectorizers initialized: count and tfidf. I've limited both to consider only n-grams that appear at least ten times. I've also limited the feature set into the top 1,000 n-grams that appear the most often in the reviews. Also it only extracts unigrams to trigrams. You can edit any of the parameters.\n",
    "\n",
    "### You can switch between count and tfidf vectorizers by changing between \"vectorizer\" and \"tfvectorizer\" in one of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "with open(\"C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/review_business_df.pkl\", 'rb') as picklefile: \n",
    "    review_business_df = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_df = review_business_df.sample(10000,random_state=1).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that I've only extract 10,000 sample reviews. This is just to manage its tracktability. Lemmatizing takes a really long time. Indirectly, 10,000 reviews is also big enough relative to the 1,000 features (discussed above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_df = sample_df.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_df['spell'] = short_df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_df['s_spell_lem'] = short_df['spell'].apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = short_df[['s_spell_lem','stars_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp = list(new_df.s_spell_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can change the vectorizer type in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature = tfvectorizer.fit_transform(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Turns count/tfidf matrix into a dataframe\n",
    "featuredf = pd.DataFrame(feature.A, columns=tfvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Concat Y and X\n",
    "new_df = pd.concat((new_df,featuredf),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 3002 entries, s_spell_lem to young\n",
      "dtypes: float64(3000), object(2)\n",
      "memory usage: 229.0+ MB\n"
     ]
    }
   ],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def string(x):\n",
    "    if x == 5:\n",
    "        return \"five\"\n",
    "    elif x == 4:\n",
    "        return 'four'\n",
    "    elif x == 3:\n",
    "        return 'three'\n",
    "    elif x == 2:\n",
    "        return 'two'\n",
    "    elif x == 1:\n",
    "        return 'one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df['stars_x'] = short_df['stars_x'].apply(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_spell_lem</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>100</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>ache</th>\n",
       "      <th>across</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>...</th>\n",
       "      <th>would definitely</th>\n",
       "      <th>would recommend</th>\n",
       "      <th>wrap</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i go out to eat often and have been to many p...</td>\n",
       "      <td>four</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126137</td>\n",
       "      <td>0.103447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>always happysatisfied when i leave they have ...</td>\n",
       "      <td>five</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dont like fried chicken there i said it sou...</td>\n",
       "      <td>five</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love this little hidden gem best go by far in...</td>\n",
       "      <td>five</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a bit conflict about this cafe because last n...</td>\n",
       "      <td>three</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         s_spell_lem stars_x  100  able  \\\n",
       "0   i go out to eat often and have been to many p...    four  0.0   0.0   \n",
       "1   always happysatisfied when i leave they have ...    five  0.0   0.0   \n",
       "2   i dont like fried chicken there i said it sou...    five  0.0   0.0   \n",
       "3   love this little hidden gem best go by far in...    five  0.0   0.0   \n",
       "4   a bit conflict about this cafe because last n...   three  0.0   0.0   \n",
       "\n",
       "   absolutely  ache    across  actually  add     added  ...    \\\n",
       "0         0.0   0.0  0.126137  0.103447  0.0  0.000000  ...     \n",
       "1         0.0   0.0  0.000000  0.000000  0.0  0.000000  ...     \n",
       "2         0.0   0.0  0.000000  0.000000  0.0  0.000000  ...     \n",
       "3         0.0   0.0  0.000000  0.000000  0.0  0.000000  ...     \n",
       "4         0.0   0.0  0.000000  0.000000  0.0  0.062492  ...     \n",
       "\n",
       "   would definitely  would recommend  wrap  wrong  year  yelp  yes  yet  york  \\\n",
       "0               0.0              0.0   0.0    0.0   0.0   0.0  0.0  0.0   0.0   \n",
       "1               0.0              0.0   0.0    0.0   0.0   0.0  0.0  0.0   0.0   \n",
       "2               0.0              0.0   0.0    0.0   0.0   0.0  0.0  0.0   0.0   \n",
       "3               0.0              0.0   0.0    0.0   0.0   0.0  0.0  0.0   0.0   \n",
       "4               0.0              0.0   0.0    0.0   0.0   0.0  0.0  0.0   0.0   \n",
       "\n",
       "   young  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 3002 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('C:/Users/kenndanielso/Documents/Github/mcnulty_yelp/data/kenn_review_df.pkl', 'wb') as picklefile: \n",
    "    pickle.dump(new_df, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Split into train and test at 75/25\n",
    "from sklearn.cross_validation import train_test_split \n",
    "train, test = train_test_split(new_df,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Split X & Y\n",
    "X_train = train.iloc[:,2:]\n",
    "Y_train = train.iloc[:,1]\n",
    "X_test = test.iloc[:,2:]\n",
    "Y_test = test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy:  0.542\n",
      "[[145  35  23  25  21]\n",
      " [ 54  50  59  54  17]\n",
      " [ 27  25 100 174  46]\n",
      " [  6   5  40 414 315]\n",
      " [ 14   3  19 183 646]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## NaiveBayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB().fit(X_train,Y_train)\n",
    "nb_Y_pred = nb.predict(X_test)\n",
    "print(\"NB Accuracy: \",np.mean(nb_Y_pred == np.array(Y_test)))\n",
    "print(confusion_matrix(Y_test,nb_Y_pred,labels=['one','two','three','four','five']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistc Accuracy:  0.5372\n",
      "[[152  32  18  22  25]\n",
      " [ 43  59  52  53  27]\n",
      " [ 17  30 116 150  59]\n",
      " [  2  12  61 370 335]\n",
      " [ 14   2  18 185 646]]\n"
     ]
    }
   ],
   "source": [
    "## Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression().fit(X_train,Y_train)\n",
    "log_Y_pred = log.predict(X_test)\n",
    "print(\"Logistc Accuracy: \",np.mean(log_Y_pred == np.array(Y_test)))\n",
    "print(confusion_matrix(Y_test,log_Y_pred,labels=['one','two','three','four','five']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Linear Accuracy:  0.5368\n",
      "[[143  50  17  20  19]\n",
      " [ 47  79  47  44  17]\n",
      " [ 22  43 125 130  52]\n",
      " [  8  17  69 377 309]\n",
      " [ 16   6  30 195 618]]\n"
     ]
    }
   ],
   "source": [
    "## Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "svcl = SVC(kernel='linear').fit(X_train,Y_train)\n",
    "svcl_Y_pred = svcl.predict(X_test)\n",
    "print(\"SVC Linear Accuracy: \",np.mean(svcl_Y_pred == np.array(Y_test)))\n",
    "print(confusion_matrix(Y_test,svcl_Y_pred,labels=['one','two','three','four','five']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC RBF Accuracy:  0.5104\n",
      "[[110  14  10  59  56]\n",
      " [ 27  22  41 102  42]\n",
      " [  8   9  61 227  67]\n",
      " [  1   3  12 403 361]\n",
      " [  4   1   2 178 680]]\n"
     ]
    }
   ],
   "source": [
    "## RBF SVC\n",
    "from sklearn.svm import SVC\n",
    "svcrbf = SVC(kernel='rbf', gamma=1).fit(X_train,Y_train)\n",
    "svcrbf_Y_pred = svcrbf.predict(X_test)\n",
    "print(\"SVC RBF Accuracy: \",np.mean(svcrbf_Y_pred == np.array(Y_test)))\n",
    "print(confusion_matrix(Y_test,svcrbf_Y_pred,labels=['one','two','three','four','five']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.3784\n",
      "[[ 92  36  40  43  38]\n",
      " [ 38  38  54  59  45]\n",
      " [ 37  61  85 106  83]\n",
      " [ 42  48 115 265 310]\n",
      " [ 42  30  74 253 466]]\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train,Y_train)\n",
    "dt_Y_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy: \",np.mean(dt_Y_pred == np.array(Y_test)))\n",
    "print(confusion_matrix(Y_test,dt_Y_pred,labels=['one','two','three','four','five']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests Accuracy:  0.436\n",
      "[[101  15  27  46  60]\n",
      " [ 43  17  36  72  66]\n",
      " [ 21  14  68 152 117]\n",
      " [ 14   7  63 265 431]\n",
      " [ 11   4  23 188 639]]\n"
     ]
    }
   ],
   "source": [
    "## Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train,Y_train)\n",
    "rf_Y_pred = rf.predict(X_test)\n",
    "print(\"Random Forests Accuracy: \",np.mean(rf_Y_pred == np.array(Y_test)))\n",
    "print(confusion_matrix(Y_test,rf_Y_pred,labels=['one','two','three','four','five']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
